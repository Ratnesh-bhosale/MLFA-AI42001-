# -*- coding: utf-8 -*-
"""Untitled23.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1CEBTHBjd5uMTPTa88Klr2S8u49qDTRwo
"""

# -*- coding: utf-8 -*-
"""ratn.bhosale@gmail.com_Sohan_Patnaik_DecisionTrees.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/18vrcHzOjfxs-Nch0i2We8RKzpCjMb14f
"""

#Run this cell
#Importing necessary libraries
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import json
ans=[0]*5

#Import the dataset and define the feature as well as the target datasets / columns

df = pd.read_csv('zoo.csv')

#We drop the animal names since this is not a good feature to split the data on.

df.drop(columns = 'animal_name', axis = 0, inplace = True)
target = df['class_type']
features = df.drop(['class_type'], axis = 1)

#Write a function to find the entropy on a split "target_col"
#def entropy(target_col):

def entropy(target_col):

    elements,counts = np.unique(target_col,return_counts = True)
    entropy = np.sum([(-counts[i]/np.sum(counts))*np.log2(counts[i]/np.sum(counts)) for i in range(len(elements))])
    return entropy

#Find the entropy of all the features in the dataset
#Save all the feature names in an array "feature names"
feature_names=['hair','feathers','eggs','milk','airborne','aquatic','predator','toothed','backbone',
               'breathes','venomous','fins','legs','tail','domestic','catsize']

entropies = {}

for feature in feature_names:
    entropies[feature] = entropy(feature)

#Find the entropy of the feature "toothed"
ans[0]= entropies['toothed']

#Write a function to calculate Information Gain on a split attribute and a target column
def InfoGain(data,split_attribute_name,target_name="class"):
    #Calculate the entropy of the total dataset
    total_entropy = entropy(data[target_name])
    #Calculate the values and the corresponding counts for the split attribute
    vals, counts= np.unique(data[split_attribute_name],return_counts=True)
    #Calculate the weighted entropy
    split_entropy = np.sum([(counts[i]/np.sum(counts))*entropy(data.where(data[split_attribute_name]==vals[i]).dropna()[target_name]) for i in range(len(vals))])
    #Calculate the information gain
    info_gain = total_entropy - split_entropy
    return info_gain

#Find the information gain having split attribute "hair" and the target feature name "milk"
ans[1]= InfoGain(df, 'hair', target_name = 'milk')

#Find the Info gain having "milk" as the split attribute and all the other features as target features one at a time

feature_info_gain = {}

for feature in feature_names:
    if feature != 'milk':
        feature_info_gain[feature] = InfoGain(df, feature, "milk")

#Import Decision Tree Classifier from sklearn
from sklearn.tree import DecisionTreeClassifier
#Split the given data into 80 percent training data and 20 percent testing data
from sklearn.model_selection import train_test_split
x_train, x_test, y_train, y_test = train_test_split(features, target, test_size = 0.2, random_state = 42)

#Fit the given data
tree = DecisionTreeClassifier(random_state = 42)
tree.fit(x_train, y_train)

#Make a prediction on the test data and return the percentage of accuracy

from sklearn.metrics import accuracy_score
y_pred_test = tree.predict(x_test)
accuracy = accuracy_score(y_pred_test, y_test)
ans[2]= accuracy * 100

#Run this cell to visualize the decision tree
from sklearn.externals.six import StringIO
from IPython.display import Image
from sklearn.tree import export_graphviz
import pydotplus

dot_data = StringIO()
export_graphviz(tree, out_file=dot_data, feature_names=feature_names,
                filled=True, rounded=True,
                special_characters=True)
graph = pydotplus.graph_from_dot_data(dot_data.getvalue())
Image(graph.create_png())

#Use sklearn to make a classification report and a confusion matrix
from sklearn.metrics import classification_report, confusion_matrix
cr = classification_report(y_test, y_pred_test, output_dict = True)
cm = confusion_matrix(y_test, y_pred_test)

#Find the recall,f1-score for class type '3'
ans[3] = (cr['3']['recall'], cr['3']['f1-score'])

#Calculate Mean Absolute Error,Mean Squared Error and Root Mean Squared Error
from sklearn.metrics import mean_squared_error, mean_absolute_error
mae = mean_absolute_error(y_test, y_pred_test)
mse = mean_squared_error(y_test, y_pred_test)
rmse = np.sqrt(mean_squared_error(y_test, y_pred_test))

#Find the mean absolute error and root mean square error, save then in a list [mae,rmse]
ans[4]= [mae, rmse]

##do not change this code
import json
ans = [str(item) for item in ans]
#print(ans)
filename = "group57_ratn.bhosale@gmail.com_Ratnesh_Bhosale_DecisionTrees"

# Eg if your name is Saurav Joshi and email id is sauravjoshi123@gmail.com, filename becomes
# filename = sauravjoshi123@gmail.com_Saurav_Joshi_LinearRegression

"""## Do not change anything below!!
- Make sure you have changed the above variable "filename" with the correct value. Do not change anything below!!
"""

from importlib import import_module
import os
from pprint import pprint

findScore = import_module('findScore')
response = findScore.main(ans)
response['details'] = filename
with open(f'evaluation_{filename}.json', 'w') as outfile:
    json.dump(response, outfile)
pprint(response)
